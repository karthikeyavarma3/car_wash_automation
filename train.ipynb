{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "import cv2\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "from keras.models import Model\n",
    "from keras.applications import MobileNet, EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Ford' , 'Honday' , 'Hyundai' , 'Nissan' ,'Renault' , 'Suzuki' , 'Tata' , 'Toyota' , 'Volkswagen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = r'C:\\Projects\\e-AUTO\\eAuto_photos\\eAuto_photos\\photos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in classes:\n",
    "    path = os.path.join(dir , category)\n",
    "    label = classes.index(category)\n",
    "    \n",
    "    \n",
    "\n",
    "    for img in os.listdir(path):\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            image = cv2.imread(os.path.join(path , img))\n",
    "            if image is not None:\n",
    "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                img_resize = cv2.resize(image_rgb , (224,224))\n",
    "                \n",
    "            else:\n",
    "                print(f\"Error loading image: {os.path.join(path, img)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {os.path.join(path, img)}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        training_data.append([img_resize, label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6460"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features , label in training_data:\n",
    "    x.append(features)\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(x)\n",
    "Y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pickle_out \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mx.pickle\u001b[39m\u001b[39m'\u001b[39m , \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m pickle\u001b[39m.\u001b[39;49mdump(X , pickle_out)\n\u001b[0;32m      3\u001b[0m pickle_out\u001b[39m.\u001b[39mclose()\n\u001b[0;32m      5\u001b[0m \u001b[39m# pickle_out = open('y.pickle' , 'wb')\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# pickle.dump(Y , pickle_out)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# pickle_out.close()\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pickle_out = open('x.pickle' , 'wb')\n",
    "pickle.dump(X , pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('y.pickle' , 'wb')\n",
    "pickle.dump(Y , pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Model Summary:\n",
    "\n",
    "This code implements a transfer learning approach using the MobileNet model for image classification. MobileNet is a pre-trained deep learning model that has been trained on the ImageNet dataset, which contains a wide range of images from various categories.\n",
    "\n",
    "In the code, the MobileNet model is loaded with pre-trained weights using the MobileNet(weights='imagenet') function. The model's top layer (fully connected layer) is excluded by setting include_top=False, as we intend to add your our final layer for classification. The input shape is specified as (224, 224, 3) to match the expected input dimensions of the MobileNet model.\n",
    "\n",
    "To fine-tune the model, the code freezes the first 15 layers of the base MobileNet model by setting their trainable attribute to False. This means that during training, only the remaining layers after the 15th layer will be updated with new weights.\n",
    "\n",
    "The data is preprocessed using an ImageDataGenerator instance, which performs various augmentation techniques such as rescaling the pixel values. The augmented data is then divided into training and validation subsets using the validation_split argument.\n",
    "\n",
    "The model is compiled with the sparse categorical cross-entropy loss function (loss='sparse_categorical_crossentropy') and the Adam optimizer (optimizer='adam'). The chosen metrics for evaluation are accuracy (metrics=['accuracy'])."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNet(weights='imagenet' , include_top=False , input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenet_1.00_224\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 112, 112, 32)      864       \n",
      "                                                                 \n",
      " conv1_bn (BatchNormalizatio  (None, 112, 112, 32)     128       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " conv1_relu (ReLU)           (None, 112, 112, 32)      0         \n",
      "                                                                 \n",
      " conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)     288       \n",
      "                                                                 \n",
      " conv_dw_1_bn (BatchNormaliz  (None, 112, 112, 32)     128       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_1_relu (ReLU)       (None, 112, 112, 32)      0         \n",
      "                                                                 \n",
      " conv_pw_1 (Conv2D)          (None, 112, 112, 64)      2048      \n",
      "                                                                 \n",
      " conv_pw_1_bn (BatchNormaliz  (None, 112, 112, 64)     256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_1_relu (ReLU)       (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " conv_pad_2 (ZeroPadding2D)  (None, 113, 113, 64)      0         \n",
      "                                                                 \n",
      " conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)       576       \n",
      "                                                                 \n",
      " conv_dw_2_bn (BatchNormaliz  (None, 56, 56, 64)       256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_2_relu (ReLU)       (None, 56, 56, 64)        0         \n",
      "                                                                 \n",
      " conv_pw_2 (Conv2D)          (None, 56, 56, 128)       8192      \n",
      "                                                                 \n",
      " conv_pw_2_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_2_relu (ReLU)       (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)      1152      \n",
      "                                                                 \n",
      " conv_dw_3_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_3_relu (ReLU)       (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv_pw_3 (Conv2D)          (None, 56, 56, 128)       16384     \n",
      "                                                                 \n",
      " conv_pw_3_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_3_relu (ReLU)       (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv_pad_4 (ZeroPadding2D)  (None, 57, 57, 128)       0         \n",
      "                                                                 \n",
      " conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)      1152      \n",
      "                                                                 \n",
      " conv_dw_4_bn (BatchNormaliz  (None, 28, 28, 128)      512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_4_relu (ReLU)       (None, 28, 28, 128)       0         \n",
      "                                                                 \n",
      " conv_pw_4 (Conv2D)          (None, 28, 28, 256)       32768     \n",
      "                                                                 \n",
      " conv_pw_4_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_4_relu (ReLU)       (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)      2304      \n",
      "                                                                 \n",
      " conv_dw_5_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_5_relu (ReLU)       (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv_pw_5 (Conv2D)          (None, 28, 28, 256)       65536     \n",
      "                                                                 \n",
      " conv_pw_5_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_5_relu (ReLU)       (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv_pad_6 (ZeroPadding2D)  (None, 29, 29, 256)       0         \n",
      "                                                                 \n",
      " conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)      2304      \n",
      "                                                                 \n",
      " conv_dw_6_bn (BatchNormaliz  (None, 14, 14, 256)      1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_6_relu (ReLU)       (None, 14, 14, 256)       0         \n",
      "                                                                 \n",
      " conv_pw_6 (Conv2D)          (None, 14, 14, 512)       131072    \n",
      "                                                                 \n",
      " conv_pw_6_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_6_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
      "                                                                 \n",
      " conv_dw_7_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_7_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_7 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_7_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_7_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
      "                                                                 \n",
      " conv_dw_8_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_8_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_8 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_8_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_8_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
      "                                                                 \n",
      " conv_dw_9_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_9_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_9 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_9_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_9_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_10 (DepthwiseConv2D  (None, 14, 14, 512)      4608      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_10_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_10_relu (ReLU)      (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_10 (Conv2D)         (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_10_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_10_relu (ReLU)      (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_11 (DepthwiseConv2D  (None, 14, 14, 512)      4608      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_11_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_11_relu (ReLU)      (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_11 (Conv2D)         (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_11_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_11_relu (ReLU)      (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)      0         \n",
      "                                                                 \n",
      " conv_dw_12 (DepthwiseConv2D  (None, 7, 7, 512)        4608      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_12_bn (BatchNormali  (None, 7, 7, 512)        2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_12_relu (ReLU)      (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " conv_pw_12 (Conv2D)         (None, 7, 7, 1024)        524288    \n",
      "                                                                 \n",
      " conv_pw_12_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_12_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " conv_dw_13 (DepthwiseConv2D  (None, 7, 7, 1024)       9216      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_13_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " conv_pw_13 (Conv2D)         (None, 7, 7, 1024)        1048576   \n",
      "                                                                 \n",
      " conv_pw_13_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,228,864\n",
      "Trainable params: 3,206,976\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[1:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in base_model.layers[15:]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([])\n",
    "model.add(base_model)\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(256 , activation='relu' , kernel_regularizer = l2(0.01)))\n",
    "# model.add(Dropout(0.3))\n",
    "model.add(Dense(128 , activation='relu' , kernel_regularizer = l2(0.01)))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(64 , activation='relu' , kernel_regularizer=l2(0.01)))\n",
    "\n",
    "\n",
    "# model.add(keras.layers.Dense(128 , activation='relu'))\n",
    "model.add(keras.layers.Dense(9 , activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenet_1.00_224 (Functio  (None, 7, 7, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 50176)             0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 256)               12845312  \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 9)                 1161      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,108,233\n",
      "Trainable params: 16,082,889\n",
      "Non-trainable params: 25,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_point = ModelCheckpoint(\n",
    "    filepath = 'best_model.1',\n",
    "    monitor = 'val_accuracy',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=3,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    \n",
    "    rescale=1/255.0,\n",
    "    validation_split=0.2\n",
    "    # rotation_range=10,\n",
    "    # width_shift_range=0.2,\n",
    "    # height_shift_range=0.2,\n",
    "    # shear_range=0.2,\n",
    "    # zoom_range=[0.9,1.2],\n",
    "    # horizontal_flip=False,\n",
    "    # fill_mode = 'reflect',\n",
    "    # brightness_range=(0.8,1.2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6179 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    dir,\n",
    "    target_size = (224,224),\n",
    "    batch_size=32,\n",
    "    color_mode='rgb',\n",
    "    class_mode = 'sparse',\n",
    "    subset = 'training'\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ford': 0, 'Honday': 1, 'Hyundai': 2, 'Nissan': 3, 'Renault': 4, 'Suzuki': 5, 'Tata': 6, 'Toyota': 7, 'Volkswagen': 8}\n"
     ]
    }
   ],
   "source": [
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1541 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = datagen.flow_from_directory(\n",
    "    dir,\n",
    "    target_size = (224,224),\n",
    "    batch_size=32,\n",
    "    color_mode='rgb',\n",
    "    class_mode = 'sparse',\n",
    "    subset='validation'\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization_strength = 0.01\n",
    "# for layer in updated_model.layers:\n",
    "#     if isinstance(layer, Dense):\n",
    "#         layer.kernel_regularizer = keras.regularizers.l2(regularization_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy' , optimizer='adam' , metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  6/194 [..............................] - ETA: 7:50 - loss: 5.1719 - accuracy: 0.8438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - ETA: 0s - loss: 1.2475 - accuracy: 0.9743\n",
      "Epoch 1: val_accuracy did not improve from 0.94549\n",
      "194/194 [==============================] - 544s 3s/step - loss: 1.2475 - accuracy: 0.9743 - val_loss: 0.6635 - val_accuracy: 0.9429\n",
      "Epoch 2/10\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.5686 - accuracy: 0.9717\n",
      "Epoch 2: val_accuracy did not improve from 0.94549\n",
      "194/194 [==============================] - 453s 2s/step - loss: 0.5686 - accuracy: 0.9717 - val_loss: 1.8481 - val_accuracy: 0.7339\n",
      "Epoch 3/10\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.4039 - accuracy: 0.9751\n",
      "Epoch 3: val_accuracy did not improve from 0.94549\n",
      "194/194 [==============================] - 503s 3s/step - loss: 0.4039 - accuracy: 0.9751 - val_loss: 0.3538 - val_accuracy: 0.9429\n",
      "Epoch 4/10\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.9943\n",
      "Epoch 4: val_accuracy improved from 0.94549 to 0.98183, saving model to best_model.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenet_1.00_224_input with unsupported characters which will be renamed to mobilenet_1_00_224_input in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - 545s 3s/step - loss: 0.1739 - accuracy: 0.9943 - val_loss: 0.1841 - val_accuracy: 0.9818\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - ETA: 0s - loss: 0.1695 - accuracy: 0.9934\n",
      "Epoch 5: val_accuracy did not improve from 0.98183\n",
      "194/194 [==============================] - 459s 2s/step - loss: 0.1695 - accuracy: 0.9934 - val_loss: 0.1943 - val_accuracy: 0.9695\n",
      "Epoch 6/10\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.1359 - accuracy: 0.9929\n",
      "Epoch 6: val_accuracy did not improve from 0.98183\n",
      "194/194 [==============================] - 473s 2s/step - loss: 0.1359 - accuracy: 0.9929 - val_loss: 0.4414 - val_accuracy: 0.9358\n",
      "Epoch 7/10\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.9861\n",
      "Epoch 7: val_accuracy did not improve from 0.98183\n",
      "194/194 [==============================] - 432s 2s/step - loss: 0.2495 - accuracy: 0.9861 - val_loss: 0.2459 - val_accuracy: 0.9676\n",
      "Epoch 7: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ba64e59450>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data = validation_generator,\n",
    "    callbacks=[early_stopping,check_point]\n",
    "    \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Name : best_model.1\n",
    "\n",
    "Evaluation report :\n",
    "\n",
    "Epoch 1:\n",
    "\n",
    "    Loss: 1.2475\n",
    "    Accuracy: 0.9743\n",
    "    Validation Loss: 0.6635\n",
    "    Validation Accuracy: 0.9429\n",
    "\n",
    "Epoch 2:\n",
    "\n",
    "    Loss: 0.5686\n",
    "    Accuracy: 0.9717\n",
    "    Validation Loss: 1.8481\n",
    "    Validation Accuracy: 0.7339\n",
    "\n",
    "Epoch 3:\n",
    "\n",
    "    Loss: 0.4039\n",
    "    Accuracy: 0.9751\n",
    "    Validation Loss: 0.3538\n",
    "    Validation Accuracy: 0.9429\n",
    "\n",
    "Epoch 4:\n",
    "\n",
    "    Loss: 0.1739\n",
    "    Accuracy: 0.9943\n",
    "    Validation Loss: 0.1841\n",
    "    Validation Accuracy: 0.9818\n",
    "\n",
    "Epoch 5:\n",
    "\n",
    "    Loss: 0.1695\n",
    "    Accuracy: 0.9934\n",
    "    Validation Loss: 0.1943\n",
    "    Validation Accuracy: 0.9695\n",
    "\n",
    "Epoch 6:\n",
    "\n",
    "    Loss: 0.1359\n",
    "    Accuracy: 0.9929\n",
    "    Validation Loss: 0.4414\n",
    "    Validation Accuracy: 0.9358\n",
    "    \n",
    "Epoch 7:\n",
    "\n",
    "    Loss: 0.2495\n",
    "    Accuracy: 0.9861\n",
    "    Validation Loss: 0.2459\n",
    "    Validation Accuracy: 0.9676\n",
    "\n",
    "\n",
    "So, this model achieved a maximum validation accuracy of 98 percent and this is considered to be the best model of all the other models I trained."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I would chose to deploy this model in production because:\n",
    "\n",
    "    Performance : \n",
    "A validation accuracy of 98% indicates that my model is able to make accurate predictions on unseen data. This level of accuracy suggests that my model has learned meaningful patterns and can effectively classify the car images based on their logos.\n",
    "\n",
    "    Efficient Architecture: \n",
    "MobileNet+(my_fully_connected_layers) is designed to be a lightweight architecture suitable for mobile and resource-constrained environments. It strikes a balance between model size and accuracy, making it computationally efficient and faster to deploy compared to larger models with similar performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 114ms/step\n",
      "Suzuki\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(r'C:\\Projects\\e-AUTO\\suzuki.jpg')\n",
    "img = cv2.cvtColor(img , cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img , (224,224))\n",
    "img = img/255\n",
    "img = np.expand_dims(img , axis=0)\n",
    "\n",
    "predictions = model.predict(img)\n",
    "\n",
    "output = np.argmax(predictions[0])\n",
    "class_ = classes[output]\n",
    "print(class_)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top-2 Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model too is based on the MobileNet architecture, which is a lightweight convolutional neural network (CNN) designed for efficient image classification tasks. The MobileNet model consists of depthwise separable convolutions, which significantly reduce the number of parameters while maintaining good accuracy. In the code, the pre-trained MobileNet model with weights from the ImageNet dataset is used as the base model. The first 15 layers of the base model are frozen, meaning they are not trainable, while the remaining layers are made trainable to adapt the model to the specific task at hand.\n",
    "\n",
    "\n",
    "The \"model2\" is a custom model built on top of the base MobileNet model. It starts with the base model's output and adds additional layers to perform further feature extraction and classification. In the code, a Flatten layer is added to convert the 4D output of the base model into a 2D feature vector. This is followed by three Dense layers with ReLU activation, which serve as fully connected layers for learning complex patterns in the features. Each Dense layer has kernel regularization (L2) applied to control overfitting. The final Dense layer has 9 units with softmax activation, representing the probabilities of the input belonging to each class in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNet(weights='imagenet' , include_top=False , input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[1:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in base_model.layers[15:]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Sequential([])\n",
    "model2.add(base_model)\n",
    "model2.add(keras.layers.Flatten())\n",
    "model2.add(keras.layers.Dense(256 , activation='relu' , kernel_regularizer = l2(0.01)))\n",
    "# model.add(Dropout(0.3))\n",
    "model2.add(Dense(128 , activation='relu' , kernel_regularizer = l2(0.01)))\n",
    "# model.add(Dropout(0.2))\n",
    "model2.add(Dense(64 , activation='relu' , kernel_regularizer=l2(0.01)))\n",
    "\n",
    "\n",
    "# model.add(keras.layers.Dense(128 , activation='relu'))\n",
    "model2.add(Dense(9 , activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6179 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    dir,\n",
    "    target_size = (224,224),\n",
    "    batch_size=32,\n",
    "    color_mode='rgb',\n",
    "    class_mode = 'sparse',\n",
    "    subset = 'training'\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1541 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = datagen.flow_from_directory(\n",
    "    dir,\n",
    "    target_size = (224,224),\n",
    "    batch_size=32,\n",
    "    color_mode='rgb',\n",
    "    class_mode = 'sparse',\n",
    "    subset='validation'\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_point = ModelCheckpoint(\n",
    "    filepath = 'best_model.2',\n",
    "    monitor = 'val_accuracy',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss = 'sparse_categorical_crossentropy' , optimizer='adam' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      " 10/194 [>.............................] - ETA: 8:33 - loss: 0.3609 - accuracy: 0.9812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - ETA: 0s - loss: 0.6032 - accuracy: 0.9547\n",
      "Epoch 1: val_accuracy did not improve from 0.97469\n",
      "194/194 [==============================] - 583s 3s/step - loss: 0.6032 - accuracy: 0.9547 - val_loss: 0.6109 - val_accuracy: 0.9156\n",
      "Epoch 2/7\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.5464 - accuracy: 0.9621 \n",
      "Epoch 2: val_accuracy did not improve from 0.97469\n",
      "194/194 [==============================] - 6327s 33s/step - loss: 0.5464 - accuracy: 0.9621 - val_loss: 0.5735 - val_accuracy: 0.9215\n",
      "Epoch 3/7\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.3436 - accuracy: 0.9804\n",
      "Epoch 3: val_accuracy did not improve from 0.97469\n",
      "194/194 [==============================] - 575s 3s/step - loss: 0.3436 - accuracy: 0.9804 - val_loss: 0.6730 - val_accuracy: 0.9150\n",
      "Epoch 4/7\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.4299 - accuracy: 0.9717\n",
      "Epoch 4: val_accuracy did not improve from 0.97469\n",
      "194/194 [==============================] - 648s 3s/step - loss: 0.4299 - accuracy: 0.9717 - val_loss: 0.7393 - val_accuracy: 0.8787\n",
      "Epoch 5/7\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.9913\n",
      "Epoch 5: val_accuracy did not improve from 0.97469\n",
      "194/194 [==============================] - 789s 4s/step - loss: 0.2473 - accuracy: 0.9913 - val_loss: 0.3340 - val_accuracy: 0.9624\n",
      "Epoch 6/7\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.3826 - accuracy: 0.9749\n",
      "Epoch 6: val_accuracy did not improve from 0.97469\n",
      "194/194 [==============================] - 1340s 7s/step - loss: 0.3826 - accuracy: 0.9749 - val_loss: 0.3825 - val_accuracy: 0.9526\n",
      "Epoch 7/7\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.2442 - accuracy: 0.9867\n",
      "Epoch 7: val_accuracy did not improve from 0.97469\n",
      "194/194 [==============================] - 742s 4s/step - loss: 0.2442 - accuracy: 0.9867 - val_loss: 0.3441 - val_accuracy: 0.9604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ba7c88c370>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(\n",
    "    train_generator,\n",
    "    epochs=7,\n",
    "    validation_data = validation_generator,\n",
    "    callbacks=[early_stopping,check_point],\n",
    "    batch_size=64\n",
    "    \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model name : mobilenet_extension.2\n",
    "\n",
    "So, this model achieved a maximum validation accuracy of 97 percent and this is considered to be the second best model of all the other models I trained.\n",
    "\n",
    "*I would also suggest to chose to deploy this model in production because:\n",
    "\n",
    "    Performance : \n",
    "A validation accuracy of 97% indicates that my model is able to make accurate predictions on unseen data. This model is also equally performing well on unseen data.\n",
    "\n",
    "    Efficient Architecture: \n",
    "MobileNet+(my_fully_connected_layers) is designed to be a lightweight architecture suitable for mobile and resource-constrained environments. It strikes a balance between model size and accuracy, making it computationally efficient and faster to deploy compared to larger models with similar performance.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top-3 Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model too is based on the MobileNet architecture, which is a lightweight convolutional neural network (CNN) designed for efficient image classification tasks. The MobileNet model consists of depthwise separable convolutions, which significantly reduce the number of parameters while maintaining good accuracy. In the code, the pre-trained MobileNet model with weights from the ImageNet dataset is used as the base model. The first 15 layers of the base model are frozen, meaning they are not trainable, while the remaining layers are made trainable to adapt the model to the specific task at hand.\n",
    "\n",
    "\n",
    "The \"model3\" is a custom model built on top of the base MobileNet model. It starts with the base model's output and adds additional layers to perform further feature extraction and classification. In the code, a Flatten layer is added to convert the 4D output of the base model into a 2D feature vector. This is followed by three Dense layers,128 128 and 64 units respectively with ReLU activation, which serve as fully connected layers for learning complex patterns in the features. Each Dense layer has kernel regularization (L2) applied to control overfitting. The final Dense layer has 9 units with softmax activation, representing the probabilities of the input belonging to each class in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNet(weights='imagenet' , include_top=False , input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[1:10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in base_model.layers[10:]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6179 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    dir,\n",
    "    target_size = (224,224),\n",
    "    batch_size=32,\n",
    "    color_mode='rgb',\n",
    "    class_mode = 'sparse',\n",
    "    subset = 'training'\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1541 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = datagen.flow_from_directory(\n",
    "    dir,\n",
    "    target_size = (224,224),\n",
    "    batch_size=32,\n",
    "    color_mode='rgb',\n",
    "    class_mode = 'sparse',\n",
    "    subset='validation'\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_point = ModelCheckpoint(\n",
    "    filepath = 'best_model.3',\n",
    "    monitor = 'val_accuracy',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = keras.Sequential([])\n",
    "model3.add(base_model)\n",
    "model3.add(keras.layers.Flatten())\n",
    "model3.add(keras.layers.Dense(128 , activation='relu' , kernel_regularizer = l2(0.01)))\n",
    "# model.add(Dropout(0.3))\n",
    "model3.add(Dense(128 , activation='relu' , kernel_regularizer = l2(0.01)))\n",
    "# model.add(Dropout(0.2))\n",
    "model3.add(Dense(64 , activation='relu' , kernel_regularizer=l2(0.01)))\n",
    "model3.add(Dense(32 , activation = 'relu' , kernel_regularizer=l2(0.01)))\n",
    "\n",
    "\n",
    "\n",
    "model3.add(Dense(9 , activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss = 'sparse_categorical_crossentropy' , optimizer='adam' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 21/194 [==>...........................] - ETA: 7:12 - loss: 8.3958 - accuracy: 0.1577"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - ETA: 0s - loss: 4.1818 - accuracy: 0.5370\n",
      "Epoch 1: val_accuracy improved from -inf to 0.29851, saving model to best_model.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenet_1.00_224_input with unsupported characters which will be renamed to mobilenet_1_00_224_input in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - 568s 3s/step - loss: 4.1818 - accuracy: 0.5370 - val_loss: 6.3561 - val_accuracy: 0.2985\n",
      "Epoch 2/10\n",
      " 30/194 [===>..........................] - ETA: 5:53 - loss: 1.7606 - accuracy: 0.8479"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - ETA: 0s - loss: 1.4830 - accuracy: 0.8581\n",
      "Epoch 2: val_accuracy improved from 0.29851 to 0.63076, saving model to best_model.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenet_1.00_224_input with unsupported characters which will be renamed to mobilenet_1_00_224_input in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - 469s 2s/step - loss: 1.4830 - accuracy: 0.8581 - val_loss: 2.6152 - val_accuracy: 0.6308\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - ETA: 0s - loss: 1.0382 - accuracy: 0.9142\n",
      "Epoch 3: val_accuracy improved from 0.63076 to 0.85269, saving model to best_model.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenet_1.00_224_input with unsupported characters which will be renamed to mobilenet_1_00_224_input in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - 753s 4s/step - loss: 1.0382 - accuracy: 0.9142 - val_loss: 1.1628 - val_accuracy: 0.8527\n",
      "Epoch 4/10\n",
      "  5/194 [..............................] - ETA: 10:11 - loss: 0.8327 - accuracy: 0.9375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - ETA: 0s - loss: 0.7669 - accuracy: 0.9510\n",
      "Epoch 4: val_accuracy did not improve from 0.85269\n",
      "194/194 [==============================] - 584s 3s/step - loss: 0.7669 - accuracy: 0.9510 - val_loss: 1.5106 - val_accuracy: 0.7982\n",
      "Epoch 5/10\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.6953 - accuracy: 0.9531\n",
      "Epoch 5: val_accuracy improved from 0.85269 to 0.86762, saving model to best_model.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenet_1.00_224_input with unsupported characters which will be renamed to mobilenet_1_00_224_input in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - 476s 2s/step - loss: 0.6953 - accuracy: 0.9531 - val_loss: 1.0689 - val_accuracy: 0.8676\n",
      "Epoch 6/10\n",
      "  2/194 [..............................] - ETA: 8:36 - loss: 0.7275 - accuracy: 0.9688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - ETA: 0s - loss: 0.7062 - accuracy: 0.9469\n",
      "Epoch 6: val_accuracy improved from 0.86762 to 0.90006, saving model to best_model.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenet_1.00_224_input with unsupported characters which will be renamed to mobilenet_1_00_224_input in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - 485s 2s/step - loss: 0.7062 - accuracy: 0.9469 - val_loss: 0.9046 - val_accuracy: 0.9001\n",
      "Epoch 7/10\n",
      "  5/194 [..............................] - ETA: 7:54 - loss: 0.6455 - accuracy: 0.9688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - ETA: 0s - loss: 0.5727 - accuracy: 0.9639\n",
      "Epoch 7: val_accuracy improved from 0.90006 to 0.92408, saving model to best_model.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenet_1.00_224_input with unsupported characters which will be renamed to mobilenet_1_00_224_input in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - 398s 2s/step - loss: 0.5727 - accuracy: 0.9639 - val_loss: 0.6489 - val_accuracy: 0.9241\n",
      "Epoch 8/10\n",
      "  2/194 [..............................] - ETA: 5:44 - loss: 0.4479 - accuracy: 0.9688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - ETA: 0s - loss: 0.4106 - accuracy: 0.9851\n",
      "Epoch 8: val_accuracy did not improve from 0.92408\n",
      "194/194 [==============================] - 397s 2s/step - loss: 0.4106 - accuracy: 0.9851 - val_loss: 1.5606 - val_accuracy: 0.7962\n",
      "Epoch 9/10\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.5833 - accuracy: 0.9689\n",
      "Epoch 9: val_accuracy did not improve from 0.92408\n",
      "194/194 [==============================] - 359s 2s/step - loss: 0.5833 - accuracy: 0.9689 - val_loss: 0.5917 - val_accuracy: 0.9228\n",
      "Epoch 10/10\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.3598 - accuracy: 0.9814\n",
      "Epoch 10: val_accuracy did not improve from 0.92408\n",
      "194/194 [==============================] - 375s 2s/step - loss: 0.3598 - accuracy: 0.9814 - val_loss: 0.6809 - val_accuracy: 0.8968\n",
      "Epoch 10: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bb0df55ab0>"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data = validation_generator,\n",
    "    callbacks=[early_stopping,check_point],\n",
    "    batch_size=64\n",
    "    \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model name : mobilenet_extension.3\n",
    "\n",
    "So, this model achieved a maximum validation accuracy of 92 percent and this is considered to be the 3rd best model of all the other models I trained.\n",
    "\n",
    "Evaluation metrics : val_accuracy\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
